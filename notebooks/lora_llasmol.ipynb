{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0e9580-35ee-4fea-8491-d39fbdb49e23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! pip install transformers==4.38.1\n",
    "# ! pip install rdkit==2023.9.4\n",
    "# ! pip install accelerate==0.27.2\n",
    "# ! pip install tensorflow==2.10.0\n",
    "# ! pip install flash-attn\n",
    "# ! pip install -q -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e66824-9e1a-420b-bea7-60f78d92747a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! pip install datasets\n",
    "# ! pip install loralib\n",
    "# ! pip install git+https://github.com/huggingface/peft.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2423ce8-51f2-4648-9f80-bf777f3cbea9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e0d485-3f89-408c-9714-c1d267b9c780",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad75ec25-7f1c-4e95-812c-33f8cd675e4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "from peft import PeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, BitsAndBytesConfig, DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bb2602-709b-4480-93f0-1888edb9f705",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HF_CREDENTIALS = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8c1cc9-079a-422a-9a38-df26f941eebe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2', token=HF_CREDENTIALS) #, model_max_length=1024)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac800906-b6f3-40ee-8c4a-3427b21319cc",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a016bf-0595-4bba-8584-095eb934f2a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat = [\n",
    "  {\"role\": \"user\", \"content\": \"\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"\"}\n",
    "]\n",
    "\n",
    "tokenizer.apply_chat_template(chat, tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef71754-78a6-4e43-81e9-ded83bab2f70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('train_nelson_data.pkl', 'rb') as f:\n",
    "    conversations = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e9e2b8-71b4-4b42-8862-dd77e0794fe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('smolinstruct/train/property_prediction-bbbp.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        if random.random() < 0.5:\n",
    "            continue\n",
    "        txt = json.loads(line)\n",
    "        chat[0]['content'] = f\"Is blood-brain barrier permeability (BBBP) a property of <SMILES> {txt['input']} </SMILES>?\"\n",
    "        chat[1]['content'] = f\"<BOOLEAN> {txt['output']} </BOOLEAN>\"\n",
    "        conversations.append(tokenizer.apply_chat_template(chat, tokenize=False))\n",
    "print(conversations[-1])\n",
    "print(len(conversations))\n",
    "\n",
    "with open('smolinstruct/train/property_prediction-clintox.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        if random.random() < 0.5:\n",
    "            continue\n",
    "        txt = json.loads(line)\n",
    "        chat[0]['content'] = f\"Is <SMILES> {txt['input']} </SMILES> toxic?\"\n",
    "        chat[1]['content'] = f\"<BOOLEAN> {txt['output']} </BOOLEAN>\"\n",
    "        conversations.append(tokenizer.apply_chat_template(chat, tokenize=False))\n",
    "print(conversations[-1])\n",
    "print(len(conversations))\n",
    "\n",
    "with open('smolinstruct/train/property_prediction-esol.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        if random.random() < 0.5:\n",
    "            continue\n",
    "        txt = json.loads(line)\n",
    "        chat[0]['content'] = f\"How soluble is <SMILES> {txt['input']} </SMILES>?\"\n",
    "        chat[1]['content'] = f\"Its log solubility is <NUMBER> {txt['output']} </NUMBER> mol/L\"\n",
    "        conversations.append(tokenizer.apply_chat_template(chat, tokenize=False))\n",
    "print(conversations[-1])\n",
    "print(len(conversations))\n",
    "\n",
    "with open('smolinstruct/train/property_prediction-hiv.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        if random.random() < 0.95:\n",
    "            continue\n",
    "        txt = json.loads(line)\n",
    "        chat[0]['content'] = f\"Can <SMILES> {txt['input']} </SMILES> serve as an inhibitor of HIV replication?\"\n",
    "        chat[1]['content'] = f\"<BOOLEAN> {txt['output']} </BOOLEAN>\"\n",
    "        conversations.append(tokenizer.apply_chat_template(chat, tokenize=False))\n",
    "print(conversations[-1])\n",
    "print(len(conversations))\n",
    "\n",
    "with open('smolinstruct/train/property_prediction-lipo.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        if random.random() < 0.9:\n",
    "            continue\n",
    "        txt = json.loads(line)\n",
    "        chat[0]['content'] = f\"Predict the octanol/water distribution coefficient logD under the circumstances of pH 7.4 for <SMILES> {txt['input']} </SMILES>\"\n",
    "        chat[1]['content'] = f\"<NUMBER> {txt['output']} </NUMBER>\"\n",
    "        conversations.append(tokenizer.apply_chat_template(chat, tokenize=False))\n",
    "print(conversations[-1])\n",
    "print(len(conversations))\n",
    "\n",
    "with open('smolinstruct/train/property_prediction-sider.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        if random.random() < 0.5:\n",
    "            continue\n",
    "        txt = json.loads(line)\n",
    "        chat[0]['content'] = f\"Are there any known side effects of <SMILES> {txt['input']} </SMILES> affecting the heart?\"\n",
    "        chat[1]['content'] = f\"<BOOLEAN> {txt['output']['Vascular disorders']} </BOOLEAN>\"\n",
    "        conversations.append(tokenizer.apply_chat_template(chat, tokenize=False))\n",
    "print(conversations[-1])\n",
    "print(len(conversations))\n",
    "\n",
    "with open('smolinstruct/train/molecule_captioning.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        if random.random() < 0.98:\n",
    "            continue\n",
    "        txt = json.loads(line)\n",
    "        chat[0]['content'] = f\"Describe the molecule: <SMILES> {txt['input']} </SMILES>\"\n",
    "        chat[1]['content'] = f\"{txt['output']}\"\n",
    "        conversations.append(tokenizer.apply_chat_template(chat, tokenize=False))\n",
    "print(conversations[-1])\n",
    "print(len(conversations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49523691-7f6b-42c0-aec1-757cbbff1027",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(conversations, padding=True, return_tensors='pt')\n",
    "dataset = Dataset.from_dict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311ad1ec-8e1c-47cb-ade0-0baacb642854",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_CREDENTIALS = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148ff4ca-8bb2-46d4-b11b-b1cda67ca20d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6dbd20-00dc-4a2a-a417-7b6d5bc2031f",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9869181a-95d1-4aef-aaa6-a3dd356a7bbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-v0.1',\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    token=HF_CREDENTIALS\n",
    ")\n",
    "\n",
    "model = PeftModelForCausalLM.from_pretrained(\n",
    "    model,\n",
    "    'osunlp/LlaSMol-Mistral-7B',\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a82ba78-01ad-4416-ac82-0ccb89836d41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d6d2c6-9efc-4452-a105-fe78cbcdb679",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False  # freeze the model - train adapters later\n",
    "    if param.ndim == 1:\n",
    "        # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "        param.data = param.data.to(torch.float32)\n",
    "\n",
    "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "    def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4934af-a863-4769-963e-2a82b6ee952b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    # target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "\n",
    "base_model_with_new_adapter = get_peft_model(model, lora_config)\n",
    "base_model_with_new_adapter.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073abf08-4097-4c2f-b918-7cc664c4867a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87ddacf-90ac-4e0e-ae2f-fd196b927550",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99958b69-3322-4d9c-89ef-7f954858f0ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607ac73a-a847-435c-864e-7ed99eb57a79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=8,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        warmup_ratio=0.1,\n",
    "        max_steps=-1,\n",
    "        learning_rate=1e-5,\n",
    "        max_grad_norm=0.5,\n",
    "        fp16=False,\n",
    "        bf16=True,\n",
    "        logging_steps=16,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        group_by_length=False,\n",
    "        output_dir='outputs'\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72b5357-e3a4-404d-8d49-086b3f204351",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.push_to_hub(\"luispintoc/llasmol-v2\",\n",
    "                  use_auth_token=True,\n",
    "                  commit_message=\"Train all - 1 epoch\",\n",
    "                  private=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc3805a-78e6-455d-a3f3-fa57f44a5efd",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929789a1-7f3f-44b3-9950-606c28201056",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5676fd-961f-4922-a18e-e0343a463b67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f272b1b1-deaf-4f0f-9d75-e353aaacba4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('val_nelson_data.pkl', 'rb') as f:\n",
    "    conversations = pickle.load(f) #3400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cafdf4b-3483-4b7e-afe5-f7d396cc1249",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_input = conversations[-1].split('[/INST]')[0] + '[/INST]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8848eeb1-66ca-4968-8d51-268eeabfe8d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff99df9-bf9e-4eec-aedf-14e8d896ce5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversations[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f2041d-b32f-4436-b949-eb2f91318636",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat = conversations[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5747da0b-d534-4c4c-8ac2-2175e587d8f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat = '<s>[INST] You are asked to check the following list of candidate SMILES and rate with Yes or No if they are less soluble to water and lower permeability than their parent SMILES <SMILES> CC1(C)CN(C(=O)Cn2ccc(=O)[nH]c2=O)C1(C)C </SMILES>. Here is the list of candidates:\\n1. <SMILES> Nc1ccn(C(=O)Cn2ccc(=O)[nH]c2=O)c(=O)n1 </SMILES> \\n2. <SMILES> CC1(C)CN(c2oc(N)c(-n3ccc(=O)[nH]c3=O)c2)C1(C)C </SMILES> \\n3. <SMILES> CC1(C)CN(C(=O)CC(CC[S@](C)=O)=O)C1(C)C </SMILES> \\n4. <SMILES> c1(-c2cccnc2)nc(C)nc2c1CCN(C(=O)NCc1ccncc1)C2 </SMILES> [/INST]1. <BOOLEAN> Yes </BOOLEAN> \\n2. <BOOLEAN> No </BOOLEAN> \\n3. <BOOLEAN> No </BOOLEAN></s>'\n",
    "prompt_input = chat.split('[/INST]')[0] + '[/INST]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39f7cc1-63da-4ce4-8820-e19ae6815e2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92b2762-743a-4cd6-97ab-2ee389084584",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt_input, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = tokenizer.decode(model.generate(**inputs, **model_config)[0])\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d020bd33-7d5d-4aea-a094-d98058306dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676eead5-f2b7-4deb-a4d0-cd843c9d809a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
