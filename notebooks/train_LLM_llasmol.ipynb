{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! pip install transformers==4.38.1\n",
    "# ! pip install rdkit==2023.9.4\n",
    "# ! pip install accelerate==0.27.2\n",
    "# ! pip install flash-attn\n",
    "# ! pip install -q -U bitsandbytes\n",
    "# ! pip install datasets\n",
    "# ! pip install loralib\n",
    "# ! pip install git+https://github.com/huggingface/peft.git\n",
    "# ! pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install tensorflow==2.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random, pickle, json, os\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import bitsandbytes as bnb\n",
    "from peft import PeftModelForCausalLM\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from math import sqrt\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "import sys\n",
    "sys.path.append('../credentials/')\n",
    "from HF_credentials import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, AutoConfig, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2', token=HF_CREDENTIALS, model_max_length=256, add_prefix_space=False)\n",
    "llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "llm_tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST]  [/INST]</s>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = [\n",
    "  {\"role\": \"user\", \"content\": \"\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"\"}\n",
    "]\n",
    "\n",
    "llm_tokenizer.apply_chat_template(chat, tokenize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_datasets(split='train'):\n",
    "\n",
    "    conversations = []\n",
    "    input_smiles = []\n",
    "\n",
    "    with open(f'../data/LlaSMol/{split}/property_prediction-bbbp.jsonl', 'r') as f:\n",
    "        for line in f:\n",
    "            txt = json.loads(line)\n",
    "            chat[0]['content'] = f\"Is blood-brain barrier permeability (BBBP) a property of <SMILES> {txt['input']} </SMILES>?\"\n",
    "            chat[1]['content'] = f\"<BOOLEAN> {txt['output']} </BOOLEAN>\"\n",
    "            # conversations.append(llm_tokenizer.apply_chat_template(chat, tokenize=True, truncation=True, padding='max_length', max_length=256))\n",
    "            conversations.append(llm_tokenizer.apply_chat_template(chat, tokenize=False))\n",
    "            input_smiles.append(txt['input'])\n",
    "    print(conversations[-1])\n",
    "\n",
    "    with open(f'../data/LlaSMol/{split}/property_prediction-clintox.jsonl', 'r') as f:\n",
    "        for line in f:\n",
    "            txt = json.loads(line)\n",
    "            chat[0]['content'] = f\"Is <SMILES> {txt['input']} </SMILES> toxic?\"\n",
    "            chat[1]['content'] = f\"<BOOLEAN> {txt['output']} </BOOLEAN>\"\n",
    "            # conversations.append(llm_tokenizer.apply_chat_template(chat, tokenize=True, truncation=True, padding='max_length', max_length=256))\n",
    "            conversations.append(llm_tokenizer.apply_chat_template(chat, tokenize=False))\n",
    "            input_smiles.append(txt['input'])\n",
    "    print(conversations[-1])\n",
    "\n",
    "    with open(f'../data/LlaSMol/{split}/property_prediction-esol.jsonl', 'r') as f:\n",
    "        for line in f:\n",
    "            txt = json.loads(line)\n",
    "            chat[0]['content'] = f\"How soluble is <SMILES> {txt['input']} </SMILES>?\"\n",
    "            chat[1]['content'] = f\"Its log solubility is <NUMBER> {txt['output']} </NUMBER> mol/L\"\n",
    "            # conversations.append(llm_tokenizer.apply_chat_template(chat, tokenize=True, truncation=True, padding='max_length', max_length=256))\n",
    "            conversations.append(llm_tokenizer.apply_chat_template(chat, tokenize=False))\n",
    "            input_smiles.append(txt['input'])\n",
    "    print(conversations[-1])\n",
    "\n",
    "    with open(f'../data/LlaSMol/{split}/property_prediction-hiv.jsonl', 'r') as f:\n",
    "        for line in f:\n",
    "            txt = json.loads(line)\n",
    "            chat[0]['content'] = f\"Can <SMILES> {txt['input']} </SMILES> serve as an inhibitor of HIV replication?\"\n",
    "            chat[1]['content'] = f\"<BOOLEAN> {txt['output']} </BOOLEAN>\"\n",
    "            # conversations.append(llm_tokenizer.apply_chat_template(chat, tokenize=True, truncation=True, padding='max_length', max_length=256))\n",
    "            conversations.append(llm_tokenizer.apply_chat_template(chat, tokenize=False))\n",
    "            input_smiles.append(txt['input'])\n",
    "    print(conversations[-1])\n",
    "\n",
    "    with open(f'../data/LlaSMol/{split}/property_prediction-lipo.jsonl', 'r') as f:\n",
    "        for line in f:\n",
    "            txt = json.loads(line)\n",
    "            chat[0]['content'] = f\"Predict the octanol/water distribution coefficient logD under the circumstances of pH 7.4 for <SMILES> {txt['input']} </SMILES>\"\n",
    "            chat[1]['content'] = f\"<NUMBER> {txt['output']} </NUMBER>\"\n",
    "            # conversations.append(llm_tokenizer.apply_chat_template(chat, tokenize=True, truncation=True, padding='max_length', max_length=256))\n",
    "            conversations.append(llm_tokenizer.apply_chat_template(chat, tokenize=False))\n",
    "            input_smiles.append(txt['input'])\n",
    "    print(conversations[-1])\n",
    "\n",
    "    with open(f'../data/LlaSMol/{split}/property_prediction-sider.jsonl', 'r') as f:\n",
    "        for line in f:\n",
    "            txt = json.loads(line)\n",
    "            chat[0]['content'] = f\"Are there any known side effects of <SMILES> {txt['input']} </SMILES> affecting the heart?\"\n",
    "            chat[1]['content'] = f\"<BOOLEAN> {txt['output']['Vascular disorders']} </BOOLEAN>\"\n",
    "            # conversations.append(llm_tokenizer.apply_chat_template(chat, tokenize=True, truncation=True, padding='max_length', max_length=256))\n",
    "            conversations.append(llm_tokenizer.apply_chat_template(chat, tokenize=False))\n",
    "            input_smiles.append(txt['input'])\n",
    "    print(conversations[-1])\n",
    "    print(len(conversations))\n",
    "\n",
    "    return conversations, input_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "<s>[INST] Is blood-brain barrier permeability (BBBP) a property of <SMILES> CC1(C)NC(=O)C(/C=C/C2=CC=CC=C2)O1 </SMILES>? [/INST]<BOOLEAN> Yes </BOOLEAN></s>\n",
      "<s>[INST] Is <SMILES> CCCCOCCOCCOCC1=CC2=C(C=C1CCC)OCO2 </SMILES> toxic? [/INST]<BOOLEAN> No </BOOLEAN></s>\n",
      "<s>[INST] How soluble is <SMILES> OCC1=CC=CC=C1OC1OC(CO)C(O)C(O)C1O </SMILES>? [/INST]Its log solubility is <NUMBER> -0.85 </NUMBER> mol/L</s>\n",
      "<s>[INST] Can <SMILES> COC(=O)C1C2=C(CC3C4=CC=CC=C4NC(=O)C31)C1=CC=CC=C1N2 </SMILES> serve as an inhibitor of HIV replication? [/INST]<BOOLEAN> No </BOOLEAN></s>\n",
      "<s>[INST] Predict the octanol/water distribution coefficient logD under the circumstances of pH 7.4 for <SMILES> C[C@H]1O[C@@H](N2C=NC3=C(N)N=C(OCC45CC6CC(CC(C6)C4)C5)N=C32)[C@H](O)[C@@H]1O </SMILES> [/INST]<NUMBER> 3.58 </NUMBER></s>\n",
      "<s>[INST] Are there any known side effects of <SMILES> CCCCCOC(=O)NC1=NC(=O)N([C@@H]2O[C@H](C)[C@@H](O)[C@H]2O)C=C1F </SMILES> affecting the heart? [/INST]<BOOLEAN> Yes </BOOLEAN></s>\n",
      "40966\n",
      "Val:\n",
      "<s>[INST] Is blood-brain barrier permeability (BBBP) a property of <SMILES> COC1=CC=C(CC2=NC=CC3=CC(OC)=C(OC)C=C23)C=C1OC </SMILES>? [/INST]<BOOLEAN> No </BOOLEAN></s>\n",
      "<s>[INST] Is <SMILES> O=C1N(CCC[NH+]2CCN(C3=CC=CC(Cl)=C3)CC2)N=C2C=CC=CN12 </SMILES> toxic? [/INST]<BOOLEAN> No </BOOLEAN></s>\n",
      "<s>[INST] How soluble is <SMILES> C1=CC=C2C(=C1)NC1=CC=CC=C12 </SMILES>? [/INST]Its log solubility is <NUMBER> -5.27 </NUMBER> mol/L</s>\n",
      "<s>[INST] Can <SMILES> CCOC(=O)C(=CN1C(=O)C(=CC2=CC=CC([N+](=O)[O-])=C2)SC1=S)C(=O)C1=CC=CC=C1 </SMILES> serve as an inhibitor of HIV replication? [/INST]<BOOLEAN> No </BOOLEAN></s>\n",
      "<s>[INST] Predict the octanol/water distribution coefficient logD under the circumstances of pH 7.4 for <SMILES> ClC1=CC(NC2=NC=NC3=C2C(OCCN2CCCC2)=NN3)=CC=C1OCC1=CC=CC=N1 </SMILES> [/INST]<NUMBER> 2.87 </NUMBER></s>\n",
      "<s>[INST] Are there any known side effects of <SMILES> O=C(NC1=C(Cl)C=NC=C1Cl)C1=CC=C(OC(F)F)C(OCC2CC2)=C1 </SMILES> affecting the heart? [/INST]<BOOLEAN> No </BOOLEAN></s>\n",
      "5117\n",
      "Test:\n",
      "<s>[INST] Is blood-brain barrier permeability (BBBP) a property of <SMILES> COC1=CC=C2OC3=CC=CC=C3C=C(N3CCN(C)CC3)C2=C1 </SMILES>? [/INST]<BOOLEAN> Yes </BOOLEAN></s>\n",
      "<s>[INST] Is <SMILES> CC(C)[C@@]1(C(=O)N[C@H]2CC(=O)O[C@]2(O)CF)CC(C2=NC=CC3=CC=CC=C23)=NO1 </SMILES> toxic? [/INST]<BOOLEAN> Yes </BOOLEAN></s>\n",
      "<s>[INST] How soluble is <SMILES> CC12CCC3C(CCC4=CC(=O)CCC43C)C1CCC2=O </SMILES>? [/INST]Its log solubility is <NUMBER> -3.69 </NUMBER> mol/L</s>\n",
      "<s>[INST] Can <SMILES> O=[N+]([O-])C1=CC=C([N+]2=CC=C3C=CC=CC3=C2)C([N+](=O)[O-])=C1 </SMILES> serve as an inhibitor of HIV replication? [/INST]<BOOLEAN> No </BOOLEAN></s>\n",
      "<s>[INST] Predict the octanol/water distribution coefficient logD under the circumstances of pH 7.4 for <SMILES> CCN(C(=O)C1=C(NS(=O)(=O)C2=CC=C(C3=CN=CO3)C=C2)N(C2=CC=CC=C2)N=C1)C1CCCCC1 </SMILES> [/INST]<NUMBER> 1.46 </NUMBER></s>\n",
      "<s>[INST] Are there any known side effects of <SMILES> OC(CN1C=NC=N1)(CN1C=NC=N1)C1=CC=C(F)C=C1F </SMILES> affecting the heart? [/INST]<BOOLEAN> Yes </BOOLEAN></s>\n",
      "5123\n"
     ]
    }
   ],
   "source": [
    "print('Train:')\n",
    "train_conversations, train_input_smiles = create_datasets('train')\n",
    "print('Val:')\n",
    "val_conversations, val_input_smiles = create_datasets('val')\n",
    "print('Test:')\n",
    "test_conversations, test_input_smiles = create_datasets('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CombinedDataset(Dataset):\n",
    "    def __init__(self, smiles_list, conversations, encoder_tokenizer, llm_tokenizer, max_length=256):\n",
    "        self.smiles_list = smiles_list\n",
    "        self.conversations = conversations\n",
    "        self.encoder_tokenizer = encoder_tokenizer\n",
    "        self.llm_tokenizer = llm_tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        smiles = self.smiles_list[idx]\n",
    "        smiles_encoding = self.encoder_tokenizer(smiles, return_tensors='pt', truncation=True, padding='max_length', max_length=self.max_length)\n",
    "        conversation_tokenized = self.llm_tokenizer(self.conversations[idx], truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt', add_special_tokens=False)\n",
    "        return {key: tensor[0].to('cuda') for key, tensor in smiles_encoding.items()}, conversation_tokenized.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load tokenizers\n",
    "chemberta_tokenizer = AutoTokenizer.from_pretrained('DeepChem/ChemBERTa-77M-MTR')\n",
    "mistral_tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2', add_prefix_space=False)\n",
    "mistral_tokenizer.pad_token = mistral_tokenizer.eos_token\n",
    "mistral_tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Create combined dataset\n",
    "train_dataset = CombinedDataset(train_input_smiles, train_conversations, chemberta_tokenizer, mistral_tokenizer)\n",
    "val_dataset = CombinedDataset(val_input_smiles, val_conversations, chemberta_tokenizer, mistral_tokenizer)\n",
    "test_dataset = CombinedDataset(test_input_smiles, test_conversations, chemberta_tokenizer, mistral_tokenizer)\n",
    "\n",
    "# Define DataLoader\n",
    "batch_size = 1\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# x, y = next(iter(combined_loader))\n",
    "\n",
    "# mol_encoder = AutoModel.from_pretrained(\"DeepChem/ChemBERTa-77M-MTR\")\n",
    "# llm_model = AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2',\n",
    "#             torch_dtype=torch.bfloat16,\n",
    "#             # quantization_config=bnb_config,\n",
    "#             device_map=\"auto\",\n",
    "#             token=HF_CREDENTIALS\n",
    "# )\n",
    "\n",
    "# mol_encoder(**x)['last_hidden_state'];\n",
    "# llm_model.model.embed_tokens(y['input_ids'].to('cuda'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MolEncoderLLMPipeline(nn.Module):\n",
    "    def __init__(self, lora_rank=32, lora_alpha=64):\n",
    "        super().__init__()\n",
    "        # Load molecule encoder\n",
    "        self.mol_encoder = AutoModel.from_pretrained(\"DeepChem/ChemBERTa-77M-MTR\", torch_dtype=torch.bfloat16).to('cuda')\n",
    "\n",
    "        # UNCOMMENT TO BRING DOWN FROM 15GB TO 7GB\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit= True,\n",
    "            bnb_4bit_quant_type= \"nf4\",\n",
    "            bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "            bnb_4bit_use_double_quant= True,\n",
    "        )\n",
    "        self.llm_config = AutoConfig.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2', token=HF_CREDENTIALS)\n",
    "        self.llm_model = AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2',\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            token=HF_CREDENTIALS\n",
    "        )\n",
    "\n",
    "        self.llm_model.config.use_cache = False\n",
    "        self.llm_model.config.pretraining_tp = 1\n",
    "\n",
    "        # Initialize LoRA layers for Mistral\n",
    "        self.lora_config = LoraConfig(\n",
    "            r=lora_rank,\n",
    "            lora_alpha=lora_alpha,\n",
    "            lora_dropout=0.05,\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"down_proj\", \"up_proj\"],\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "\n",
    "        # Freeze encoder and LLM weights\n",
    "        for param in self.mol_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.llm_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.linear_project = nn.Linear(self.mol_encoder.config.hidden_size, self.llm_config.hidden_size, dtype=torch.bfloat16)\n",
    "\n",
    "        # Apply LoRA modification\n",
    "        self.llm_model = get_peft_model(self.llm_model, self.lora_config)\n",
    "\n",
    "    def forward(self, smiles_tokens, text_tokens):\n",
    "        # Encoder forward pass / Get SMILES embeddings\n",
    "        mol_encoder_output = self.mol_encoder(**smiles_tokens)\n",
    "        smiles_embedding = mol_encoder_output['last_hidden_state'][:,0,:] # torch.Size([batch, max_length, 384])\n",
    "        smiles_projection = self.linear_project(smiles_embedding).unsqueeze(1)\n",
    "        # print(smiles_projection.shape)\n",
    "\n",
    "        # Get embeddings from LLM for the question\n",
    "        embedding_layer = self.llm_model.model.model.embed_tokens\n",
    "        llm_embeddings = embedding_layer(text_tokens['input_ids'].to('cuda')).squeeze(1) # torch.Size([batch, max_length, 4096])\n",
    "        # print(llm_embeddings.shape)\n",
    "\n",
    "        # Concatenate encoder and LLM embeddings\n",
    "        combined_embeddings = torch.cat((smiles_projection, llm_embeddings), dim=1)\n",
    "        # print(combined_embeddings.shape)\n",
    "\n",
    "        # Custom attention mask\n",
    "        attention_mask = torch.zeros(batch_size, combined_embeddings.shape[1], combined_embeddings.shape[1], device='cuda')\n",
    "        \n",
    "        # SMILES mask for itself\n",
    "        attention_mask[:, 0, 0] = 1\n",
    "        for i in range(1, combined_embeddings.shape[1]):\n",
    "            attention_mask[:, i, 0:i+1] = 1 # From SMILES to current token (inclusive)\n",
    "        attention_mask = attention_mask.unsqueeze(1)\n",
    "        # print(attention_mask.shape)\n",
    "\n",
    "        # Pass through Mistral's transformer layers with LoRA adjustments\n",
    "        output = self.llm_model(inputs_embeds=combined_embeddings, attention_mask=attention_mask)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = MolEncoderLLMPipeline(lora_rank=16, lora_alpha=16).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.llm_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y = next(iter(combined_loader))\n",
    "# model(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_answer(text, is_boolean):\n",
    "    if is_boolean:\n",
    "        start = text.find('<BOOLEAN>') + len('<BOOLEAN>')\n",
    "        end = text.find('</BOOLEAN>')\n",
    "        return text[start:end].strip()\n",
    "    else:\n",
    "        start = text.find('<NUMBER>') + len('<NUMBER>')\n",
    "        end = text.find('</NUMBER>')\n",
    "        return text[start:end].strip()\n",
    "\n",
    "def get_answer(true_sentence, pred_sentence):\n",
    "    true_answer = true_sentence.split('[/INST]')[1]\n",
    "    pred_answer = pred_sentence.split('[/INST]')[1]\n",
    "\n",
    "    if 'BOOLEAN' in true_answer:\n",
    "        y_true = parse_answer(true_answer, True)\n",
    "        y_pred = parse_answer(pred_answer, True)\n",
    "        return y_true, y_pred\n",
    "    \n",
    "    elif 'NUMBER' in true_answer:\n",
    "        y_true = parse_answer(true_answer, False)\n",
    "        y_pred = parse_answer(pred_answer, False)\n",
    "        return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=1e-5)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=mistral_tokenizer.pad_token_id)\n",
    "\n",
    "# Define the total number of training steps and the number of warmup steps\n",
    "epochs = 5\n",
    "total_steps = len(test_loader) * epochs\n",
    "warmup_steps = 100\n",
    "\n",
    "accumulation_steps = 32\n",
    "\n",
    "# Create the learning rate scheduler\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    tprog = tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "    for i, batch in tprog:\n",
    "        smiles_data, conversation_data = batch\n",
    "\n",
    "        # Forward pass\n",
    "        with autocast():\n",
    "            output = model(smiles_data, conversation_data)\n",
    "            logits = output.logits[:, 1:, :]\n",
    "\n",
    "            # Prepare labels\n",
    "            labels = conversation_data['input_ids'].squeeze(1)\n",
    "            labels = torch.cat([labels[:, 1:], labels.new_full((labels.size(0), 1), mistral_tokenizer.pad_token_id)], dim=1)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(logits.reshape(-1, logits.size(-1)), labels.view(-1))\n",
    "\n",
    "        # Backward and accumulate gradients\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        tprog.set_description(f'train step loss: {loss.item():.4f}')\n",
    "\n",
    "        if (i+1) % accumulation_steps == 0:  # Step the optimizer every accumulation_steps\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Step the scheduler\n",
    "            scheduler.step()\n",
    "\n",
    "            # Clean\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Validation step\n",
    "        with torch.no_grad():\n",
    "            if i % 500 == 0:\n",
    "\n",
    "                categories = [\"BBBP\", \"side effects\", \"logD\", \"soluble\", \"toxic\", \"HIV\"]\n",
    "                preds, invalid_count, trues = {cat: [] for cat in categories}, {cat: 0 for cat in categories}, {cat: [] for cat in categories}\n",
    "\n",
    "                def convert_to_boolean(input_string):\n",
    "                    return True if input_string == 'Yes' else False if input_string == 'No' else None\n",
    "\n",
    "                for batch, true_sentence in zip(test_loader, test_conversations):\n",
    "                    # Predict\n",
    "                    smiles_data, conversation_data = batch\n",
    "                    output = model(smiles_data, conversation_data)\n",
    "                    output_ids = output.logits.argmax(dim=-1)\n",
    "                    pred_sentence = mistral_tokenizer.decode(output_ids)\n",
    "                    y_true, y_pred = get_answer(true_sentence, pred_sentence)\n",
    "                    for category in categories:\n",
    "                        if category in true_sentence:\n",
    "                            if category in [\"BBBP\", \"side effects\", \"toxic\", \"HIV\"]:  # binary categories\n",
    "                                if y_pred in ['Yes', 'No']:\n",
    "                                    preds[category].append(convert_to_boolean(y_pred))\n",
    "                                    trues[category].append(convert_to_boolean(y_true))\n",
    "                                else:\n",
    "                                    invalid_count[category] += 1\n",
    "                            else:  # continuous categories\n",
    "                                try:\n",
    "                                    preds[category].append(float(y_pred))\n",
    "                                    trues[category].append(float(y_true))\n",
    "                                except:\n",
    "                                    invalid_count[category] += 1\n",
    "\n",
    "                for key in preds:\n",
    "                    if len(preds[key]) > 0:  # to avoid division by zero\n",
    "                        if key in [\"BBBP\", \"side effects\", \"toxic\", \"HIV\"]:  # binary categories\n",
    "                            accuracy = accuracy_score(trues[key], preds[key])\n",
    "                            print(f'{key} accuracy: {accuracy:.4f}')\n",
    "                        else:  # continuous categories\n",
    "                            rmse = sqrt(mean_squared_error(trues[key], preds[key]))\n",
    "                            print(f'{key} RMSE: {rmse:.4f}')\n",
    "                print('Invalid count:')\n",
    "                print(invalid_count)\n",
    "\n",
    "                # Clean\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBBP accuracy: 1.0000\n",
      "side effects accuracy: 1.0000\n",
      "logD RMSE: 0.0000\n",
      "soluble RMSE: 0.0000\n",
      "toxic accuracy: 1.0000\n",
      "HIV accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "categories = [\"BBBP\", \"side effects\", \"logD\", \"soluble\", \"toxic\", \"HIV\"]\n",
    "preds, invalid_count, trues = {cat: [] for cat in categories}, {cat: 0 for cat in categories}, {cat: [] for cat in categories}\n",
    "\n",
    "def convert_to_boolean(input_string):\n",
    "    return True if input_string == 'Yes' else False if input_string == 'No' else None\n",
    "\n",
    "for batch, true_sentence in zip(test_loader, test_conversations):\n",
    "    # Predict\n",
    "    pred_sentence = decode(...)\n",
    "    y_true, y_pred = get_answer(true_sentence, pred_sentence)\n",
    "    for category in categories:\n",
    "        if category in true_sentence:\n",
    "            if category in [\"BBBP\", \"side effects\", \"toxic\", \"HIV\"]:  # binary categories\n",
    "                if y_pred in ['Yes', 'No']:\n",
    "                    preds[category].append(convert_to_boolean(y_pred))\n",
    "                    trues[category].append(convert_to_boolean(y_true))\n",
    "                else:\n",
    "                    invalid_count[category] += 1\n",
    "            else:  # continuous categories\n",
    "                try:\n",
    "                    preds[category].append(float(y_pred))\n",
    "                    trues[category].append(float(y_true))\n",
    "                except:\n",
    "                    invalid_count[category] += 1\n",
    "\n",
    "for key in preds:\n",
    "    if len(preds[key]) > 0:  # to avoid division by zero\n",
    "        if key in [\"BBBP\", \"side effects\", \"toxic\", \"HIV\"]:  # binary categories\n",
    "            accuracy = accuracy_score(trues[key], preds[key])\n",
    "            print(f'{key} accuracy: {accuracy:.4f}')\n",
    "        else:  # continuous categories\n",
    "            rmse = sqrt(mean_squared_error(trues[key], preds[key]))\n",
    "            print(f'{key} RMSE: {rmse:.4f}')\n",
    "\n",
    "print(invalid_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break after this\n",
    "import sys\n",
    "sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBBP accuracy: 1.0000\n",
      "side effects accuracy: 1.0000\n",
      "logD RMSE: 0.0000\n",
      "soluble RMSE: 0.0000\n",
      "toxic accuracy: 1.0000\n",
      "HIV accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# # Initialize lists to store mean accuracies\n",
    "# preds = {\n",
    "#     \"BBBP\": [],\n",
    "#     \"side effects\": [],\n",
    "#     \"logD\": [],\n",
    "#     \"soluble\": [],\n",
    "#     \"toxic\": [],\n",
    "#     \"HIV\": []\n",
    "# }\n",
    "\n",
    "# invalid_count = {\n",
    "#     \"BBBP\": 0,\n",
    "#     \"side effects\": 0,\n",
    "#     \"logD\": 0,\n",
    "#     \"soluble\": 0,\n",
    "#     \"toxic\": 0,\n",
    "#     \"HIV\": 0\n",
    "# }\n",
    "\n",
    "# trues = {\n",
    "#     \"BBBP\": [],\n",
    "#     \"side effects\": [],\n",
    "#     \"logD\": [],\n",
    "#     \"soluble\": [],\n",
    "#     \"toxic\": [],\n",
    "#     \"HIV\": []\n",
    "# }\n",
    "\n",
    "# def convert_to_boolean(input_string):\n",
    "#     if input_string == 'Yes':\n",
    "#         return True\n",
    "#     elif input_string == 'No':\n",
    "#         return False\n",
    "#     else:\n",
    "#         return None\n",
    "\n",
    "# # for batch, true_sentence in zip(test_loader, test_conversations):\n",
    "# for pred_sentence, true_sentence in zip(test_conversations, test_conversations):\n",
    "\n",
    "#     # pred_sentence = decode(...)\n",
    "#     y_true, y_pred = get_answer(true_sentence, pred_sentence)\n",
    "\n",
    "#     # Determine the category based on the conversation\n",
    "#     if 'BBBP' in true_sentence:\n",
    "#         if (y_pred=='Yes') or (y_pred=='No'):\n",
    "#             preds[\"BBBP\"].append(convert_to_boolean(y_pred))\n",
    "#             trues[\"BBBP\"].append(convert_to_boolean(y_true))\n",
    "#         else:\n",
    "#             invalid_count[\"BBBP\"] += 1\n",
    "\n",
    "#     elif 'side effects' in true_sentence:\n",
    "#         if (y_pred=='Yes') or (y_pred=='No'):\n",
    "#             preds[\"side effects\"].append(convert_to_boolean(y_pred))\n",
    "#             trues[\"side effects\"].append(convert_to_boolean(y_true))\n",
    "#         else:\n",
    "#             invalid_count[\"side effects\"] += 1\n",
    "\n",
    "#     elif 'logD' in true_sentence:\n",
    "#         try:\n",
    "#             preds[\"logD\"].append(float(y_pred))\n",
    "#             trues[\"logD\"].append(float(y_true))\n",
    "#         except:\n",
    "#             invalid_count[\"logD\"] += 1\n",
    "        \n",
    "#     elif 'soluble' in true_sentence:\n",
    "#         try:\n",
    "#             preds[\"soluble\"].append(float(y_pred))\n",
    "#             trues[\"soluble\"].append(float(y_true))\n",
    "#         except:\n",
    "#             invalid_count[\"soluble\"] += 1\n",
    "\n",
    "#     elif 'toxic' in true_sentence:\n",
    "#         if (y_pred=='Yes') or (y_pred=='No'):\n",
    "#             preds[\"toxic\"].append(convert_to_boolean(y_pred))\n",
    "#             trues[\"toxic\"].append(convert_to_boolean(y_true))\n",
    "#         else:\n",
    "#             invalid_count[\"toxic\"] += 1\n",
    "\n",
    "#     elif 'HIV' in true_sentence:\n",
    "#         if (y_pred=='Yes') or (y_pred=='No'):\n",
    "#             preds[\"HIV\"].append(convert_to_boolean(y_pred))\n",
    "#             trues[\"HIV\"].append(convert_to_boolean(y_true))\n",
    "#         else:\n",
    "#             invalid_count[\"HIV\"] += 1\n",
    "\n",
    "# for key in preds:\n",
    "#     if key in [\"BBBP\", \"side effects\", \"toxic\", \"HIV\"]:  # binary categories\n",
    "#         if len(preds[key]) > 0:  # to avoid division by zero\n",
    "#             accuracy = accuracy_score(trues[key], preds[key])\n",
    "#             print(f'{key} accuracy: {accuracy:.4f}')\n",
    "#     else:  # continuous categories\n",
    "#         if len(preds[key]) > 0:  # to avoid division by zero\n",
    "#             rmse = sqrt(mean_squared_error(trues[key], preds[key]))\n",
    "#             print(f'{key} RMSE: {rmse:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
