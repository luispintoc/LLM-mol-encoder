{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! pip install transformers==4.38.1\n",
    "# ! pip install rdkit==2023.9.4\n",
    "# ! pip install accelerate==0.27.2\n",
    "# ! pip install flash-attn\n",
    "# ! pip install -q -U bitsandbytes\n",
    "# ! pip install datasets\n",
    "# ! pip install loralib\n",
    "# ! pip install git+https://github.com/huggingface/peft.git\n",
    "# ! pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install tensorflow==2.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random, pickle, json, os\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import bitsandbytes as bnb\n",
    "from peft import PeftModelForCausalLM\n",
    "\n",
    "import sys\n",
    "sys.path.append('../credentials/')\n",
    "from HF_credentials import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, AutoConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2', token=HF_CREDENTIALS, model_max_length=256, add_prefix_space=False)\n",
    "llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "llm_tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat = [\n",
    "  {\"role\": \"user\", \"content\": \"\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"\"}\n",
    "]\n",
    "\n",
    "llm_tokenizer.apply_chat_template(chat, tokenize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_datasets(split='train'):\n",
    "\n",
    "    conversations = []\n",
    "    input_smiles = []\n",
    "\n",
    "    with open(f'./data/LlaSMol/{split}/property_prediction-bbbp.jsonl', 'r') as f:\n",
    "        for line in f:\n",
    "            txt = json.loads(line)\n",
    "            chat[0]['content'] = f\"Is blood-brain barrier permeability (BBBP) a property of <SMILES> {txt['input']} </SMILES>?\"\n",
    "            chat[1]['content'] = f\"<BOOLEAN> {txt['output']} </BOOLEAN>\"\n",
    "            # conversations.append(llm_tokenizer.apply_chat_template(chat, tokenize=True, truncation=True, padding='max_length', max_length=256))\n",
    "            conversations.append(llm_tokenizer.apply_chat_template(chat, tokenize=False))\n",
    "            input_smiles.append(txt['input'])\n",
    "    print(conversations[-1])\n",
    "\n",
    "    with open(f'./data/LlaSMol/{split}/property_prediction-clintox.jsonl', 'r') as f:\n",
    "        for line in f:\n",
    "            txt = json.loads(line)\n",
    "            chat[0]['content'] = f\"Is <SMILES> {txt['input']} </SMILES> toxic?\"\n",
    "            chat[1]['content'] = f\"<BOOLEAN> {txt['output']} </BOOLEAN>\"\n",
    "            # conversations.append(llm_tokenizer.apply_chat_template(chat, tokenize=True, truncation=True, padding='max_length', max_length=256))\n",
    "            conversations.append(llm_tokenizer.apply_chat_template(chat, tokenize=False))\n",
    "            input_smiles.append(txt['input'])\n",
    "    print(conversations[-1])\n",
    "\n",
    "    with open(f'./data/LlaSMol/{split}/property_prediction-esol.jsonl', 'r') as f:\n",
    "        for line in f:\n",
    "            txt = json.loads(line)\n",
    "            chat[0]['content'] = f\"How soluble is <SMILES> {txt['input']} </SMILES>?\"\n",
    "            chat[1]['content'] = f\"Its log solubility is <NUMBER> {txt['output']} </NUMBER> mol/L\"\n",
    "            # conversations.append(llm_tokenizer.apply_chat_template(chat, tokenize=True, truncation=True, padding='max_length', max_length=256))\n",
    "            conversations.append(llm_tokenizer.apply_chat_template(chat, tokenize=False))\n",
    "            input_smiles.append(txt['input'])\n",
    "    print(conversations[-1])\n",
    "\n",
    "    with open(f'./data/LlaSMol/{split}/property_prediction-hiv.jsonl', 'r') as f:\n",
    "        for line in f:\n",
    "            txt = json.loads(line)\n",
    "            chat[0]['content'] = f\"Can <SMILES> {txt['input']} </SMILES> serve as an inhibitor of HIV replication?\"\n",
    "            chat[1]['content'] = f\"<BOOLEAN> {txt['output']} </BOOLEAN>\"\n",
    "            # conversations.append(llm_tokenizer.apply_chat_template(chat, tokenize=True, truncation=True, padding='max_length', max_length=256))\n",
    "            conversations.append(llm_tokenizer.apply_chat_template(chat, tokenize=False))\n",
    "            input_smiles.append(txt['input'])\n",
    "    print(conversations[-1])\n",
    "\n",
    "    with open(f'./data/LlaSMol/{split}/property_prediction-lipo.jsonl', 'r') as f:\n",
    "        for line in f:\n",
    "            txt = json.loads(line)\n",
    "            chat[0]['content'] = f\"Predict the octanol/water distribution coefficient logD under the circumstances of pH 7.4 for <SMILES> {txt['input']} </SMILES>\"\n",
    "            chat[1]['content'] = f\"<NUMBER> {txt['output']} </NUMBER>\"\n",
    "            # conversations.append(llm_tokenizer.apply_chat_template(chat, tokenize=True, truncation=True, padding='max_length', max_length=256))\n",
    "            conversations.append(llm_tokenizer.apply_chat_template(chat, tokenize=False))\n",
    "            input_smiles.append(txt['input'])\n",
    "    print(conversations[-1])\n",
    "\n",
    "    with open(f'./data/LlaSMol/{split}/property_prediction-sider.jsonl', 'r') as f:\n",
    "        for line in f:\n",
    "            txt = json.loads(line)\n",
    "            chat[0]['content'] = f\"Are there any known side effects of <SMILES> {txt['input']} </SMILES> affecting the heart?\"\n",
    "            chat[1]['content'] = f\"<BOOLEAN> {txt['output']['Vascular disorders']} </BOOLEAN>\"\n",
    "            # conversations.append(llm_tokenizer.apply_chat_template(chat, tokenize=True, truncation=True, padding='max_length', max_length=256))\n",
    "            conversations.append(llm_tokenizer.apply_chat_template(chat, tokenize=False))\n",
    "            input_smiles.append(txt['input'])\n",
    "    print(conversations[-1])\n",
    "    print(len(conversations))\n",
    "\n",
    "    return conversations, input_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Train:')\n",
    "train_conversations, train_input_smiles = create_datasets('train')\n",
    "print('Test:')\n",
    "test_conversations, test_input_smiles = create_datasets('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CombinedDataset(Dataset):\n",
    "    def __init__(self, smiles_list, conversations, encoder_tokenizer, llm_tokenizer, max_length=256):\n",
    "        self.smiles_list = smiles_list\n",
    "        self.conversations = conversations\n",
    "        self.encoder_tokenizer = encoder_tokenizer\n",
    "        self.llm_tokenizer = llm_tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        smiles = self.smiles_list[idx]\n",
    "        smiles_encoding = self.encoder_tokenizer(smiles, return_tensors='pt', truncation=True, padding='max_length', max_length=self.max_length)\n",
    "        conversation_tokenized = self.llm_tokenizer(self.conversations[idx], truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        return {key: tensor[0].to('cuda') for key, tensor in smiles_encoding.items()}, conversation_tokenized.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load tokenizers\n",
    "chemberta_tokenizer = AutoTokenizer.from_pretrained('DeepChem/ChemBERTa-77M-MTR')\n",
    "mistral_tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2', add_prefix_space=False)\n",
    "mistral_tokenizer.pad_token = mistral_tokenizer.eos_token\n",
    "mistral_tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Create combined dataset\n",
    "combined_dataset = CombinedDataset(test_input_smiles, test_conversations, chemberta_tokenizer, mistral_tokenizer)\n",
    "\n",
    "# Define DataLoader\n",
    "batch_size = 2\n",
    "combined_loader = DataLoader(combined_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# x, y = next(iter(combined_loader))\n",
    "\n",
    "# mol_encoder = AutoModel.from_pretrained(\"DeepChem/ChemBERTa-77M-MTR\")\n",
    "# llm_model = AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2',\n",
    "#             torch_dtype=torch.bfloat16,\n",
    "#             # quantization_config=bnb_config,\n",
    "#             device_map=\"auto\",\n",
    "#             token=HF_CREDENTIALS\n",
    "# )\n",
    "\n",
    "# mol_encoder(**x)['last_hidden_state'];\n",
    "# llm_model.model.embed_tokens(y['input_ids'].to('cuda'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MolEncoderLLMPipeline(nn.Module):\n",
    "    def __init__(self, lora_rank=32, lora_alpha=64):\n",
    "        super().__init__()\n",
    "        # Load molecule encoder\n",
    "        self.mol_encoder = AutoModel.from_pretrained(\"DeepChem/ChemBERTa-77M-MTR\").to('cuda')\n",
    "\n",
    "        # UNCOMMENT TO BRING DOWN FROM 15GB TO 7GB\n",
    "        # bnb_config = BitsAndBytesConfig(\n",
    "        #     load_in_4bit= True,\n",
    "        #     bnb_4bit_quant_type= \"nf4\",\n",
    "        #     bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "        #     bnb_4bit_use_double_quant= False,\n",
    "        # )\n",
    "        self.llm_config = AutoConfig.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2', token=HF_CREDENTIALS)\n",
    "        self.llm_model = AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2',\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            # quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            token=HF_CREDENTIALS\n",
    "        )\n",
    "\n",
    "        # Freeze encoder and LLM weights\n",
    "        for param in self.mol_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.llm_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.linear_project = nn.Linear(self.mol_encoder.config.hidden_size, self.llm_config.hidden_size)\n",
    "\n",
    "    def forward(self, smiles_tokens, text_tokens):\n",
    "        # Encoder forward pass / Get SMILES embeddings\n",
    "        mol_encoder_output = self.mol_encoder(**smiles_tokens)\n",
    "        smiles_embedding = mol_encoder_output['last_hidden_state'][:,0,:] # torch.Size([batch, max_length, 384])\n",
    "\n",
    "        # Get embeddings from LLM for the question\n",
    "        embedding_layer = self.llm_model.model.embed_tokens\n",
    "        llm_embeddings = embedding_layer(text_tokens['input_ids'].to('cuda')) # torch.Size([batch, 1, max_length, 4096])\n",
    "\n",
    "        return smiles_embedding, llm_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = MolEncoderLLMPipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y = next(iter(combined_loader))\n",
    "# model(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRA(nn.Module):\n",
    "    def __init__(self, embed_dim, rank, alpha, dropout_rate=0.05):\n",
    "        super(LoRA, self).__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha # Scaling factor for LoRA\n",
    "\n",
    "        # Low-rank matrices A and B\n",
    "        self.A = nn.Parameter(torch.randn(embed_dim, rank))\n",
    "        self.B = nn.Parameter(torch.randn(rank, embed_dim))\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, original_weight):\n",
    "        delta_weight = self.alpha * torch.matmul(self.A, self.B)\n",
    "        delta_weight = self.dropout(delta_weight)\n",
    "        return original_weight + delta_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolEncoderLLMPipeline(nn.Module):\n",
    "    def __init__(self, mol_encoder, llm_model, llm_embedding_dim, lora_rank=32, lora_alpha=64):\n",
    "        super().__init__()\n",
    "        # Load molecule encoder\n",
    "        self.mol_encoder = AutoModel.from_pretrained(\"DeepChem/ChemBERTa-77M-MTR\")\n",
    "\n",
    "        llm_config = AutoConfig.from_pretrained('mistralai/Mistral-7B-v0.1')\n",
    "        self.llm_model = AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2',\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            token=HF_CREDENTIALS\n",
    "        )\n",
    "        self.llm_model.config.use_cache = False\n",
    "        self.llm_model.config.pretraining_tp = 1\n",
    "\n",
    "        # Initialize LoRA layers for Mistral\n",
    "        self.lora_layers = nn.ModuleList([\n",
    "            LoRA(llm_config.hidden_size, lora_rank, lora_alpha) for _ in range(len(self.llm_model.encoder.layer))\n",
    "        ])\n",
    "\n",
    "        # Freeze encoder and LLM weights\n",
    "        for param in self.mol_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.llm_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, smiles_tokens, input_ids):\n",
    "        # Encoder forward pass / Get SMILES embeddings\n",
    "        mol_encoder_output = self.mol_encoder(smiles_tokens)\n",
    "        embedding = mol_encoder_output.last_hidden_states[:,0,:]\n",
    "\n",
    "        # Get embeddings from LLM for the question\n",
    "        embedding_layer = self.llm_model.embed_tokens\n",
    "        llm_embeddings = embedding_layer(input_ids)\n",
    "\n",
    "        # Concatenate encoder and LLM embeddings\n",
    "        combined_embeddings = #concat([llm_embeddings])\n",
    "\n",
    "        # Pass through Mistral's transformer layers with LoRA adjustments\n",
    "        extended_attention_mask = torch.ones(combined_embeddings.shape[0], combined_embeddings.shape[1], device=combined_embeddings.device)\n",
    "        hidden_states = combined_embeddings\n",
    "        for i, layer_module in enumerate(self.llm_model.encoder.layer):\n",
    "            layer_output = layer_module(hidden_states, attention_mask=extended_attention_mask)[0]\n",
    "            # Apply LoRA modification\n",
    "            qkv_weights = [self.lora_layers[i](w) for w in layer_module.attention.self.query.weight, layer_module.attention.self.key.weight, layer_module.attention.self.value.weight]\n",
    "            layer_module.attention.self.query.weight, layer_module.attention.self.key.weight, layer_module.attention.self.value.weight = qkv_weights\n",
    "            hidden_states = layer_output\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume model and criterion are defined elsewhere\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "epochs = 5\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    for batch in combined_loader:\n",
    "        smiles_data, conversation_data = batch\n",
    "        smiles_input_ids, smiles_attention_mask = smiles_data['input_ids'].squeeze(1), smiles_data['attention_mask'].squeeze(1)\n",
    "        convo_input_ids, convo_attention_mask = conversation_data['input_ids'].squeeze(1), conversation_data['attention_mask'].squeeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(smiles_input_ids, convo_input_ids) # Adjust if your model's `forward` method expects more parameters\n",
    "        \n",
    "        # Define labels appropriately\n",
    "        labels = ... # Define how to obtain these\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.config.use_cache = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
