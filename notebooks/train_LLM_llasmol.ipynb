{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! pip install transformers==4.38.1\n",
    "# ! pip install rdkit==2023.9.4\n",
    "# ! pip install accelerate==0.27.2\n",
    "# ! pip install flash-attn\n",
    "# ! pip install -q -U bitsandbytes\n",
    "# ! pip install datasets\n",
    "# ! pip install loralib\n",
    "# ! pip install git+https://github.com/huggingface/peft.git\n",
    "# ! pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install tensorflow==2.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random, pickle, json, os\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import bitsandbytes as bnb\n",
    "from peft import PeftModelForCausalLM\n",
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "import sys\n",
    "sys.path.append('../credentials/')\n",
    "from HF_credentials import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, AutoConfig, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2', token=HF_CREDENTIALS, model_max_length=256, add_prefix_space=False)\n",
    "llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "llm_tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat = [\n",
    "  {\"role\": \"user\", \"content\": \"\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"\"}\n",
    "]\n",
    "\n",
    "llm_tokenizer.apply_chat_template(chat, tokenize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_datasets(split='train'):\n",
    "\n",
    "    conversations = []\n",
    "    input_smiles = []\n",
    "\n",
    "    with open(f'../data/LlaSMol/{split}/property_prediction-bbbp.jsonl', 'r') as f:\n",
    "        for line in f:\n",
    "            txt = json.loads(line)\n",
    "            chat[0]['content'] = f\"Is blood-brain barrier permeability (BBBP) a property of <SMILES> {txt['input']} </SMILES>?\"\n",
    "            chat[1]['content'] = f\"<BOOLEAN> {txt['output']} </BOOLEAN>\"\n",
    "            # conversations.append(llm_tokenizer.apply_chat_template(chat, tokenize=True, truncation=True, padding='max_length', max_length=256))\n",
    "            conversations.append(llm_tokenizer.apply_chat_template(chat, tokenize=False))\n",
    "            input_smiles.append(txt['input'])\n",
    "    print(conversations[-1])\n",
    "\n",
    "    with open(f'../data/LlaSMol/{split}/property_prediction-clintox.jsonl', 'r') as f:\n",
    "        for line in f:\n",
    "            txt = json.loads(line)\n",
    "            chat[0]['content'] = f\"Is <SMILES> {txt['input']} </SMILES> toxic?\"\n",
    "            chat[1]['content'] = f\"<BOOLEAN> {txt['output']} </BOOLEAN>\"\n",
    "            # conversations.append(llm_tokenizer.apply_chat_template(chat, tokenize=True, truncation=True, padding='max_length', max_length=256))\n",
    "            conversations.append(llm_tokenizer.apply_chat_template(chat, tokenize=False))\n",
    "            input_smiles.append(txt['input'])\n",
    "    print(conversations[-1])\n",
    "\n",
    "    with open(f'../data/LlaSMol/{split}/property_prediction-esol.jsonl', 'r') as f:\n",
    "        for line in f:\n",
    "            txt = json.loads(line)\n",
    "            chat[0]['content'] = f\"How soluble is <SMILES> {txt['input']} </SMILES>?\"\n",
    "            chat[1]['content'] = f\"Its log solubility is <NUMBER> {txt['output']} </NUMBER> mol/L\"\n",
    "            # conversations.append(llm_tokenizer.apply_chat_template(chat, tokenize=True, truncation=True, padding='max_length', max_length=256))\n",
    "            conversations.append(llm_tokenizer.apply_chat_template(chat, tokenize=False))\n",
    "            input_smiles.append(txt['input'])\n",
    "    print(conversations[-1])\n",
    "\n",
    "    with open(f'../data/LlaSMol/{split}/property_prediction-hiv.jsonl', 'r') as f:\n",
    "        for line in f:\n",
    "            txt = json.loads(line)\n",
    "            chat[0]['content'] = f\"Can <SMILES> {txt['input']} </SMILES> serve as an inhibitor of HIV replication?\"\n",
    "            chat[1]['content'] = f\"<BOOLEAN> {txt['output']} </BOOLEAN>\"\n",
    "            # conversations.append(llm_tokenizer.apply_chat_template(chat, tokenize=True, truncation=True, padding='max_length', max_length=256))\n",
    "            conversations.append(llm_tokenizer.apply_chat_template(chat, tokenize=False))\n",
    "            input_smiles.append(txt['input'])\n",
    "    print(conversations[-1])\n",
    "\n",
    "    with open(f'../data/LlaSMol/{split}/property_prediction-lipo.jsonl', 'r') as f:\n",
    "        for line in f:\n",
    "            txt = json.loads(line)\n",
    "            chat[0]['content'] = f\"Predict the octanol/water distribution coefficient logD under the circumstances of pH 7.4 for <SMILES> {txt['input']} </SMILES>\"\n",
    "            chat[1]['content'] = f\"<NUMBER> {txt['output']} </NUMBER>\"\n",
    "            # conversations.append(llm_tokenizer.apply_chat_template(chat, tokenize=True, truncation=True, padding='max_length', max_length=256))\n",
    "            conversations.append(llm_tokenizer.apply_chat_template(chat, tokenize=False))\n",
    "            input_smiles.append(txt['input'])\n",
    "    print(conversations[-1])\n",
    "\n",
    "    with open(f'../data/LlaSMol/{split}/property_prediction-sider.jsonl', 'r') as f:\n",
    "        for line in f:\n",
    "            txt = json.loads(line)\n",
    "            chat[0]['content'] = f\"Are there any known side effects of <SMILES> {txt['input']} </SMILES> affecting the heart?\"\n",
    "            chat[1]['content'] = f\"<BOOLEAN> {txt['output']['Vascular disorders']} </BOOLEAN>\"\n",
    "            # conversations.append(llm_tokenizer.apply_chat_template(chat, tokenize=True, truncation=True, padding='max_length', max_length=256))\n",
    "            conversations.append(llm_tokenizer.apply_chat_template(chat, tokenize=False))\n",
    "            input_smiles.append(txt['input'])\n",
    "    print(conversations[-1])\n",
    "    print(len(conversations))\n",
    "\n",
    "    return conversations, input_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Train:')\n",
    "train_conversations, train_input_smiles = create_datasets('train')\n",
    "print('Test:')\n",
    "test_conversations, test_input_smiles = create_datasets('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CombinedDataset(Dataset):\n",
    "    def __init__(self, smiles_list, conversations, encoder_tokenizer, llm_tokenizer, max_length=256):\n",
    "        self.smiles_list = smiles_list\n",
    "        self.conversations = conversations\n",
    "        self.encoder_tokenizer = encoder_tokenizer\n",
    "        self.llm_tokenizer = llm_tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        smiles = self.smiles_list[idx]\n",
    "        smiles_encoding = self.encoder_tokenizer(smiles, return_tensors='pt', truncation=True, padding='max_length', max_length=self.max_length)\n",
    "        conversation_tokenized = self.llm_tokenizer(self.conversations[idx], truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt', add_special_tokens=False)\n",
    "        return {key: tensor[0].to('cuda') for key, tensor in smiles_encoding.items()}, conversation_tokenized.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load tokenizers\n",
    "chemberta_tokenizer = AutoTokenizer.from_pretrained('DeepChem/ChemBERTa-77M-MTR')\n",
    "mistral_tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2', add_prefix_space=False)\n",
    "mistral_tokenizer.pad_token = mistral_tokenizer.eos_token\n",
    "mistral_tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Create combined dataset\n",
    "train_dataset = CombinedDataset(train_input_smiles, train_conversations, chemberta_tokenizer, mistral_tokenizer)\n",
    "test_dataset = CombinedDataset(test_input_smiles, test_conversations, chemberta_tokenizer, mistral_tokenizer)\n",
    "\n",
    "# Define DataLoader\n",
    "batch_size = 1\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# x, y = next(iter(combined_loader))\n",
    "\n",
    "# mol_encoder = AutoModel.from_pretrained(\"DeepChem/ChemBERTa-77M-MTR\")\n",
    "# llm_model = AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2',\n",
    "#             torch_dtype=torch.bfloat16,\n",
    "#             # quantization_config=bnb_config,\n",
    "#             device_map=\"auto\",\n",
    "#             token=HF_CREDENTIALS\n",
    "# )\n",
    "\n",
    "# mol_encoder(**x)['last_hidden_state'];\n",
    "# llm_model.model.embed_tokens(y['input_ids'].to('cuda'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MolEncoderLLMPipeline(nn.Module):\n",
    "    def __init__(self, lora_rank=32, lora_alpha=64):\n",
    "        super().__init__()\n",
    "        # Load molecule encoder\n",
    "        self.mol_encoder = AutoModel.from_pretrained(\"DeepChem/ChemBERTa-77M-MTR\", torch_dtype=torch.bfloat16).to('cuda')\n",
    "\n",
    "        # UNCOMMENT TO BRING DOWN FROM 15GB TO 7GB\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit= True,\n",
    "            bnb_4bit_quant_type= \"nf4\",\n",
    "            bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "            bnb_4bit_use_double_quant= True,\n",
    "        )\n",
    "        self.llm_config = AutoConfig.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2', token=HF_CREDENTIALS)\n",
    "        self.llm_model = AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2',\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            token=HF_CREDENTIALS\n",
    "        )\n",
    "\n",
    "        self.llm_model.config.use_cache = False\n",
    "        self.llm_model.config.pretraining_tp = 1\n",
    "\n",
    "        # Initialize LoRA layers for Mistral\n",
    "        self.lora_config = LoraConfig(\n",
    "            r=lora_rank,\n",
    "            lora_alpha=lora_alpha,\n",
    "            lora_dropout=0.05,\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"down_proj\", \"up_proj\"],\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "\n",
    "        # Freeze encoder and LLM weights\n",
    "        for param in self.mol_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.llm_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.linear_project = nn.Linear(self.mol_encoder.config.hidden_size, self.llm_config.hidden_size, dtype=torch.bfloat16)\n",
    "\n",
    "        # Apply LoRA modification\n",
    "        self.llm_model = get_peft_model(self.llm_model, self.lora_config)\n",
    "\n",
    "    def forward(self, smiles_tokens, text_tokens):\n",
    "        # Encoder forward pass / Get SMILES embeddings\n",
    "        mol_encoder_output = self.mol_encoder(**smiles_tokens)\n",
    "        smiles_embedding = mol_encoder_output['last_hidden_state'][:,0,:] # torch.Size([batch, max_length, 384])\n",
    "        smiles_projection = self.linear_project(smiles_embedding).unsqueeze(1)\n",
    "        # print(smiles_projection.shape)\n",
    "\n",
    "        # Get embeddings from LLM for the question\n",
    "        embedding_layer = self.llm_model.model.model.embed_tokens\n",
    "        llm_embeddings = embedding_layer(text_tokens['input_ids'].to('cuda')).squeeze(1) # torch.Size([batch, max_length, 4096])\n",
    "        # print(llm_embeddings.shape)\n",
    "\n",
    "        # Concatenate encoder and LLM embeddings\n",
    "        combined_embeddings = torch.cat((smiles_projection, llm_embeddings), dim=1)\n",
    "        # print(combined_embeddings.shape)\n",
    "\n",
    "        # Custom attention mask\n",
    "        attention_mask = torch.zeros(batch_size, combined_embeddings.shape[1], combined_embeddings.shape[1], device='cuda')\n",
    "        \n",
    "        # SMILES mask for itself\n",
    "        attention_mask[:, 0, 0] = 1\n",
    "        for i in range(1, combined_embeddings.shape[1]):\n",
    "            attention_mask[:, i, 0:i+1] = 1 # From SMILES to current token (inclusive)\n",
    "        attention_mask = attention_mask.unsqueeze(1)\n",
    "        # print(attention_mask.shape)\n",
    "\n",
    "        # Pass through Mistral's transformer layers with LoRA adjustments\n",
    "        output = self.llm_model(inputs_embeds=combined_embeddings, attention_mask=attention_mask)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = MolEncoderLLMPipeline(lora_rank=16, lora_alpha=16).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.llm_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y = next(iter(combined_loader))\n",
    "# model(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "  \n",
    "# dataset = load_dataset('osunlp/SMolInstruct')\n",
    "# train_set = dataset['train']\n",
    "# validation_set = dataset['validation']\n",
    "# test_set = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for split, split_dataset in dataset.items():\n",
    "#     split_dataset.to_json(f\"squad-{split}.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast\n",
    "\n",
    "\n",
    "optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=1e-5)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=mistral_tokenizer.pad_token_id)\n",
    "\n",
    "# Define the total number of training steps and the number of warmup steps\n",
    "epochs = 5\n",
    "total_steps = len(test_loader) * epochs\n",
    "warmup_steps = 100\n",
    "\n",
    "accumulation_steps = 32\n",
    "\n",
    "# Create the learning rate scheduler\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    tprog = tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "    for i, batch in tprog:\n",
    "        smiles_data, conversation_data = batch\n",
    "\n",
    "        # Forward pass\n",
    "        with autocast():\n",
    "            output = model(smiles_data, conversation_data)\n",
    "            logits = output.logits[:, 1:, :]\n",
    "\n",
    "            # Prepare labels\n",
    "            labels = conversation_data['input_ids'].squeeze(1)\n",
    "            labels = torch.cat([labels[:, 1:], labels.new_full((labels.size(0), 1), mistral_tokenizer.pad_token_id)], dim=1)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(logits.reshape(-1, logits.size(-1)), labels.view(-1))\n",
    "\n",
    "        # Backward and accumulate gradients\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        tprog.set_description(f'train step loss: {loss.item():.4f}')\n",
    "\n",
    "        if (i+1) % accumulation_steps == 0:  # Step the optimizer every accumulation_steps\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Step the scheduler\n",
    "            scheduler.step()\n",
    "\n",
    "            # Clean\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_answer(text, is_boolean):\n",
    "#     if is_boolean:\n",
    "#         start = text.find('<BOOLEAN>') + len('<BOOLEAN>')\n",
    "#         end = text.find('</BOOLEAN>')\n",
    "#         return text[start:end].strip()\n",
    "#     else:\n",
    "#         start = text.find('<NUMBER>') + len('<NUMBER>')\n",
    "#         end = text.find('</NUMBER>')\n",
    "#         return text[start:end].strip()\n",
    "\n",
    "# def get_answer(true_sentence, pred_sentence):\n",
    "#     true_answer = true_sentence.split('[/INST]')[1]\n",
    "#     pred_answer = pred_sentence.split('[/INST]')[1]\n",
    "\n",
    "#     if 'BOOLEAN' in true_answer:\n",
    "#         y_true = parse_answer(true_answer, True)\n",
    "#         y_pred = parse_answer(pred_answer, True)\n",
    "#         return y_true, y_pred\n",
    "    \n",
    "#     elif 'NUMBER' in true_answer:\n",
    "#         y_true = parse_answer(true_answer, False)\n",
    "#         y_pred = parse_answer(pred_answer, False)\n",
    "#         return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = train_conversations[0]\n",
    "pred = train_conversations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_conversations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_answer(true, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize lists to store mean accuracies\n",
    "# mean_accs = {\n",
    "#     \"BBBP\": [],\n",
    "#     \"side effects\": [],\n",
    "#     \"logD\": [],\n",
    "#     \"soluble\": [],\n",
    "#     \"toxic\": [],\n",
    "#     \"HIV\": []\n",
    "# }\n",
    "\n",
    "# # Iterate through the dataset\n",
    "# for _, y in tqdm(combined_dataset):\n",
    "#     # Initialize lists to store results for each category\n",
    "#     category_results = {category: [] for category in mean_accs}\n",
    "\n",
    "#     # Iterate through ground truth and prediction\n",
    "#     for gt, pred in zip(ground_truth, prediction):\n",
    "#         # Get answer for the current category\n",
    "#         _, _, result = get_answer(gt, pred)\n",
    "#         category = None\n",
    "\n",
    "#         # Determine the category based on the conversation\n",
    "#         if 'BBBP' in pred:\n",
    "#             category = \"BBBP\"\n",
    "#         elif 'side effects' in pred:\n",
    "#             category = \"side effects\"\n",
    "#         elif 'logD' in pred:\n",
    "#             category = \"logD\"\n",
    "#         elif 'soluble' in pred:\n",
    "#             category = \"soluble\"\n",
    "#         elif 'toxic' in pred:\n",
    "#             category = \"toxic\"\n",
    "#         else:\n",
    "#             category = \"HIV\"\n",
    "\n",
    "#         # Append result to the respective category list\n",
    "#         category_results[category].append(result)\n",
    "\n",
    "#     # Calculate mean accuracy for each category and append to mean_accs\n",
    "#     for category, results in category_results.items():\n",
    "#         mean_acc = sum(results) / len(results) if results else 0\n",
    "#         mean_accs[category].append(mean_acc)\n",
    "\n",
    "# # Compute the final mean accuracies for each category\n",
    "# final_mean_accs = {category: sum(accs) / len(accs) for category, accs in mean_accs.items()}\n",
    "\n",
    "# # Print or use final mean accuracies as needed\n",
    "# for category, acc in final_mean_accs.items():\n",
    "#     print(f\"Final {category} Mean Accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize lists to store mean accuracies\n",
    "# mean_accs = {\n",
    "#     \"BBBP\": [],\n",
    "#     \"side effects\": [],\n",
    "#     \"logD\": [],\n",
    "#     \"soluble\": [],\n",
    "#     \"toxic\": [],\n",
    "#     \"HIV\": []\n",
    "# }\n",
    "\n",
    "# for batch, true_sentence in zip(test_loader, test_conversations):\n",
    "#     print(text)\n",
    "\n",
    "#     pred_sentence = decode(...)\n",
    "\n",
    "#     category_results = {category: [] for category in mean_accs}\n",
    "\n",
    "#     get_answer(true_sentence, pred_sentence)\n",
    "\n",
    "#     # Determine the category based on the conversation\n",
    "#     if 'BBBP' in true_sentence:\n",
    "#         category = \"BBBP\"\n",
    "#     elif 'side effects' in true_sentence:\n",
    "#         category = \"side effects\"\n",
    "#     elif 'logD' in true_sentence:\n",
    "#         category = \"logD\"\n",
    "#     elif 'soluble' in true_sentence:\n",
    "#         category = \"soluble\"\n",
    "#     elif 'toxic' in true_sentence:\n",
    "#         category = \"toxic\"\n",
    "#     else:\n",
    "#         category = \"HIV\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break after this\n",
    "import sys\n",
    "sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.config.use_cache = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
